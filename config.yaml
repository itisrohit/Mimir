# Mimir Configuration File
# This is the central configuration file that controls all components

# Application Settings
app:
  name: "Mimir"
  version: "1.0.0"
  debug: false
  description: "High-performance document processing and embedding pipeline"

# File paths and directories
paths:
  sessions_dir: "./.data/sessions"
  temp_dir: "./.data/temp"
  logs_dir: "./.data/logs"
  exports_dir: "./.data/exports"
  models_dir: "./models"

# Document Processing Configuration
document_processing:
  # Chunk settings
  chunk_size: 1000              # Maximum characters per chunk
  chunk_overlap: 200            # Overlap between chunks to preserve context
  clean_text: true
  preserve_formatting: false
  remove_extra_whitespace: true
  preserve_sentences: true      # Try to break at sentence boundaries
  preserve_paragraphs: true     # Try to break at paragraph boundaries
  max_file_size_mb: 100         # Maximum file size to process
  normalize_unicode: true       # Normalize Unicode characters
  
  # Supported file types
  supported_types: ["txt", "md", "pdf", "csv", "json"]
  
  # Enhanced separators for better chunking
  separators:
    - ". "      # Sentence ending with space
    - "! "      # Exclamation with space  
    - "? "      # Question with space
    - ".\n"     # Sentence ending with newline
    - "!\n"     # Exclamation with newline
    - "?\n"     # Question with newline
    - "\n\n"    # Paragraph break
    - "\n### "  # Markdown header
    - "\n## "   # Markdown header
    - "\n# "    # Markdown header
    - "\n* "    # Bullet point
    - "\n- "    # Bullet point
    - "\n"      # Simple newline (last resort)

# Embedding Configuration
embedding:
  # Model settings
  model: "models/bge-m3-onnx"
  dim: 1024                     # Embedding dimension (1024 for BGE-M3)
  batch_size: 16                # Batch size for processing
  semantic_search_enabled: true
  
  # Performance settings
  enable_caching: true
  cache_size_mb: 256
  parallel_processing: true
  max_threads: 4
  
  # Model-specific settings
  tokenizer:
    type: "sentencepiece"       # Tokenizer type (sentencepiece, onnx, etc.)
    model_path: "sentencepiece.bpe.model"
    max_length: 512             # Maximum sequence length
  
  # ONNX Runtime settings
  onnx:
    optimization_level: 1        # 0=disable, 1=basic, 2=extended
    execution_mode: "sequential" # sequential, parallel
    enable_mem_pattern: true
    enable_cpu_mem_arena: true

# Vector Database Configuration
vector_db:
  type: "faiss"                 # Options: "faiss", "qdrant", "sqlite_vss"
  
  # FAISS settings
  faiss:
    index_type: "IndexFlatIP"   # Options: "IndexFlatIP", "IndexFlatL2", "IndexIVFFlat"
    metric: "inner_product"     # Options: "inner_product", "l2"
    nlist: 100                  # For IVF indexes
    
  # Qdrant settings (for future use)
  qdrant:
    host: "localhost"
    port: 6333
    collection_name: "mimir_vectors"
    
  # SQLite with vector search (for future use)
  sqlite_vss:
    db_path: "./.data/vectors.db"

# Chat/Query Configuration
chat:
  provider: "local"             # Options: "local", "openai", "groq", "ollama"
  model: "llama2"               # Model name
  max_tokens: 2048              # Maximum response length
  temperature: 0.7              # Response creativity (0.0 - 1.0)
  
  # Context settings
  max_context_chunks: 5         # Number of relevant chunks to include
  similarity_threshold: 0.7     # Minimum similarity score for chunk inclusion
  
  # Provider-specific settings
  openai:
    api_key: ""                 # Set via environment variable
    model: "gpt-3.5-turbo"
    max_tokens: 2048
    temperature: 0.7
  
  groq:
    api_key: ""
    model: "mixtral-8x7b-32768"
    max_tokens: 2048
    temperature: 0.7
  
  ollama:
    base_url: "http://localhost:11434"
    model: "llama2"
    keep_alive: "5m"

# Logging Configuration
logging:
  level: "INFO"                 # Options: "DEBUG", "INFO", "WARN", "ERROR"
  file_logging: true
  console_logging: true
  max_log_size_mb: 10
  max_log_files: 5
  
  # Component-specific logging
  components:
    embedding: "INFO"
    document_processing: "INFO"
    session: "INFO"
    chat: "INFO"

# Performance Settings
performance:
  enable_caching: true
  cache_size_mb: 256
  parallel_processing: true
  max_threads: 4
  
  # Memory settings
  max_memory_usage_mb: 2048
  enable_memory_monitoring: true
  
  # Processing settings
  batch_processing: true
  max_batch_size: 32
  enable_profiling: false

# Export Settings
export:
  default_format: "txt"         # Options: "txt", "json", "markdown", "csv"
  include_metadata: true
  include_timestamps: true
  include_sources: true
  
  # Format-specific settings
  json:
    pretty_print: true
    include_embeddings: false   # Don't include embeddings in JSON export
  
  markdown:
    include_headers: true
    include_links: true

# Session Settings
session:
  auto_save: true               # Auto-save sessions on changes
  save_interval_minutes: 5      # Auto-save interval
  max_sessions: 100             # Maximum number of sessions to keep
  cleanup_old_sessions: false   # Automatically clean up old sessions
  max_session_age_days: 30      # Age after which sessions are considered old
  
  # Session data settings
  include_embeddings: true      # Store embeddings in session
  include_chat_history: true    # Store chat history in session
  compression_enabled: false    # Compress session data

# Development Settings
development:
  enable_debug_mode: false
  enable_profiling: false
  enable_memory_tracking: false
  log_performance_metrics: false